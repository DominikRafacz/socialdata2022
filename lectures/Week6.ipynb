{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "\n",
    "Phew. Is it week 6 already? \n",
    "\n",
    "Last week we had an intro to machine learning and regression and this week we continue with some more ML but focusing on classification instead. There are lots of courses on machine learning at DTU. And across many research areas, people use ML for all kinds of things. So there's a good chance you're already familiar with what's going to happen today. \n",
    "\n",
    "In the following, we continue introducing fundamentals of ML, decision trees and start with some prediction tasks on crime data. You might ask, why are we doing this? Well, a couple of reasons:\n",
    "\n",
    "1. It ties nicely with how we started this course: do you remember all we learnt about predictive policing in Week 1? So, today it is our turn to make predictions and see how well we can do with the data we have been exploring.\n",
    "\n",
    "2. Visualization **AND** machine learning is a powerful combination. A combination that is pretty rare. \n",
    "  - Usually it's the case that people are either good at machine learning or data viz, but not both. \n",
    "  - So what we will be able to do in this class is an unusual combo: We can use ML to understand data and then visualize the outputs of the machine-learning.\n",
    "    \n",
    "The plan for today is as follows:\n",
    "\n",
    "1. In part 1, we go more in depth on fundamentals of machine learning;\n",
    "2. In part 2, we get an introduction to Decision Trees;\n",
    "3. The next parts are about putting everything together for prediction - This part will be added soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Fundamentals of machine learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with a couple of lectures from Ole Winter about model selection and feature extraction. These connect nicely with what you should have already read in DSFS Chaper 11. If you did not read the chater yet, it is time for you to do it. \n",
    "\n",
    "Find it on DTU Learn under 'Course content' $\\rightarrow$ 'Content' $\\rightarrow$ 'Lecture 6 reading' \n",
    "\n",
    "**Model selection**\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/MHhlAtw3Ces/0.jpg)](https://www.youtube.com/watch?v=MHhlAtw3Ces)\n",
    "\n",
    "**Feature extraction and selection**\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/RZmitKn220Q/0.jpg)](https://www.youtube.com/watch?v=RZmitKn220Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 1*: A few questions about machine learning to see whether you've read the text and watched the videos. \n",
    ">\n",
    "> * What do we mean by a 'feature' in a machine learning model?\n",
    "> * What is the main problem with overfitting?\n",
    "> * Explain the connection between the bias-variance trade-off and overfitting/underfitting.\n",
    "> * The `Luke is for leukemia` on page 145 in the reading is a great example of why accuracy is not a good measure in very unbalanced problems. Try to come up with a similar example based on a different type of data (either one you are interested in or one related to the SF crime dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Decision Tree Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to decision trees. This is a fantastically useful supervised machine-learning method, that we use all the time in research. To get started on the decision trees, we asked you to read DSFS, chapter 17 (if you didn't read it you can find it in DTU Learn). \n",
    "\n",
    "And our little session on decision trees wouldn't be complete without hearing from Ole about these things. \n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/LAA_CnkAEx8/0.jpg)](https://www.youtube.com/watch?v=LAA_CnkAEx8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 2:* Just a few questions to make sure you've read the text (DSFS chapter 17) and/or watched the video.\n",
    "> \n",
    "> * There are two main kinds of decision trees depending on the type of output (numeric vs. categorical). What are they?\n",
    "> * Explain in your own words: Why is entropy useful when deciding where to split the data?\n",
    "> * Why are trees prone to overfitting?\n",
    "> * Explain (in your own words) how random forests help prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following I added some additional material for you to explore decision trees through some fantastic *visual* introductions. \n",
    "\n",
    "*Decision Trees 1*: The visual introduction to decision trees on this webpage is AMAZING. Take a look to get an intuitive feel for how trees work. Do not miss this one, it's a treat! http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "\n",
    "*Decision Trees 2*: the second part of the visual introduction is about the topic of model selection, and bias/variance tradeoffs that we looked into earlier during this lesson. But once again, here those topics are visualized in a fantastic and inspiring way, that will make it stick in your brain better. So check it out http://www.r2d3.us/visual-intro-to-machine-learning-part-2/\n",
    "\n",
    "\n",
    "\n",
    "*Decision tree tutorials*: And of course the best way to learn how to get this stuff rolling in practice, is to work through a tutorial or two. We recommend the ones below:\n",
    "  * https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    "  * https://towardsdatascience.com/random-forest-in-python-24d0893d51c0 (this one also has good considerations regarding the one-hot encodings)\n",
    "  \n",
    "(But there are many other good ones out there.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be continued ... :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
